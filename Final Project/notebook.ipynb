{"cells":[{"cell_type":"markdown","source":["# SETUP"],"metadata":{"id":"J6kSRoNh3iqW"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKqyKr-v3fE-","executionInfo":{"status":"ok","timestamp":1701573374376,"user_tz":480,"elapsed":80223,"user":{"displayName":"JINWOO BAIK","userId":"01620325729908986677"}},"outputId":"66e7fa91-f662-493c-fcea-04a6496ed4bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Collecting git+https://github.com/metadriverse/metadrive\n","  Cloning https://github.com/metadriverse/metadrive to /tmp/pip-req-build-pm51_2in\n","  Running command git clone --filter=blob:none --quiet https://github.com/metadriverse/metadrive /tmp/pip-req-build-pm51_2in\n","  Resolved https://github.com/metadriverse/metadrive to commit e001457656eddf3122682eb262fc8705d656a979\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (2.31.0)\n","Collecting gymnasium<0.29,>=0.28 (from metadrive-simulator==0.4.1.2)\n","  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<=1.24.2,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (1.23.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (3.7.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (1.5.3)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (2.5.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (4.66.1)\n","Collecting yapf (from metadrive-simulator==0.4.1.2)\n","  Downloading yapf-0.40.2-py3-none-any.whl (254 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (0.12.2)\n","Collecting progressbar (from metadrive-simulator==0.4.1.2)\n","  Downloading progressbar-2.5.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting panda3d==1.10.13 (from metadrive-simulator==0.4.1.2)\n","  Downloading panda3d-1.10.13-cp310-cp310-manylinux2014_x86_64.whl (54.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting panda3d-gltf==0.13 (from metadrive-simulator==0.4.1.2)\n","  Downloading panda3d_gltf-0.13-py3-none-any.whl (25 kB)\n","Collecting panda3d-simplepbr (from metadrive-simulator==0.4.1.2)\n","  Downloading panda3d_simplepbr-0.10-py3-none-any.whl (10 kB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (9.4.0)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (7.4.3)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (4.8.0.76)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (4.9.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (1.11.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (5.9.5)\n","Requirement already satisfied: geopandas in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (0.13.2)\n","Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (3.13.1)\n","Requirement already satisfied: Pygments in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator==0.4.1.2) (2.16.1)\n","Collecting jax-jumpy>=1.0.0 (from gymnasium<0.29,>=0.28->metadrive-simulator==0.4.1.2)\n","  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.29,>=0.28->metadrive-simulator==0.4.1.2) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.29,>=0.28->metadrive-simulator==0.4.1.2) (4.5.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium<0.29,>=0.28->metadrive-simulator==0.4.1.2)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas->metadrive-simulator==0.4.1.2) (1.9.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas->metadrive-simulator==0.4.1.2) (23.2)\n","Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas->metadrive-simulator==0.4.1.2) (3.6.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->metadrive-simulator==0.4.1.2) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->metadrive-simulator==0.4.1.2) (2023.3.post1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->metadrive-simulator==0.4.1.2) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->metadrive-simulator==0.4.1.2) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->metadrive-simulator==0.4.1.2) (4.45.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->metadrive-simulator==0.4.1.2) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->metadrive-simulator==0.4.1.2) (3.1.1)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->metadrive-simulator==0.4.1.2) (2.0.0)\n","Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->metadrive-simulator==0.4.1.2) (1.3.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->metadrive-simulator==0.4.1.2) (1.2.0)\n","Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->metadrive-simulator==0.4.1.2) (2.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->metadrive-simulator==0.4.1.2) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->metadrive-simulator==0.4.1.2) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->metadrive-simulator==0.4.1.2) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->metadrive-simulator==0.4.1.2) (2023.11.17)\n","Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->metadrive-simulator==0.4.1.2) (6.8.0)\n","Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->metadrive-simulator==0.4.1.2) (4.0.0)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas->metadrive-simulator==0.4.1.2) (23.1.0)\n","Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas->metadrive-simulator==0.4.1.2) (8.1.7)\n","Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas->metadrive-simulator==0.4.1.2) (1.1.1)\n","Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas->metadrive-simulator==0.4.1.2) (0.7.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas->metadrive-simulator==0.4.1.2) (1.16.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas->metadrive-simulator==0.4.1.2) (67.7.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->metadrive-simulator==0.4.1.2) (3.17.0)\n","Building wheels for collected packages: metadrive-simulator, progressbar\n","  Building wheel for metadrive-simulator (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for metadrive-simulator: filename=metadrive_simulator-0.4.1.2-py3-none-any.whl size=54954844 sha256=b27008cbbdf965123476508b5e223e825ad3105e6836bea5011fda6f310795be\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-03_bgltf/wheels/ef/5e/36/261da9376764e2e7f796c0d75141f5763305ce2f7b6864a845\n","  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12067 sha256=d3d13f45304d2973201ef9888a87d6669cf8f3cd9bacc9b054f9ba98a3a14294\n","  Stored in directory: /root/.cache/pip/wheels/cd/17/e5/765d1a3112ff3978f70223502f6047e06c43a24d7c5f8ff95b\n","Successfully built metadrive-simulator progressbar\n","Installing collected packages: progressbar, panda3d, farama-notifications, panda3d-simplepbr, jax-jumpy, yapf, panda3d-gltf, gymnasium, metadrive-simulator\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0 metadrive-simulator-0.4.1.2 panda3d-1.10.13 panda3d-gltf-0.13 panda3d-simplepbr-0.10 progressbar-2.5 yapf-0.40.2\n","Start to profile the efficiency of MetaDrive with 1000 maps and ~4 vehicles!\n","\u001b[38;20m[INFO] Environment: MetaDriveEnv\u001b[0m\n","\u001b[38;20m[INFO] MetaDrive version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()]\u001b[0m\n","\u001b[38;20m[INFO] Render Mode: none\u001b[0m\n","\u001b[38;20m[INFO] Horizon (Max steps per agent): None\u001b[0m\n","\u001b[33;20m[WARNING] Assets folder doesn't exist. Begin to download assets... (base_engine.py:781)\u001b[0m\n","\u001b[38;20m[INFO] Pull assets from https://github.com/metadriverse/metadrive/releases/download/MetaDrive-0.4.1.2/assets.zip to /usr/local/lib/python3.10/dist-packages/metadrive/assets.zip\u001b[0m\n","100% ||\n","\u001b[38;20m[INFO] Successfully download assets, version: 0.4.1.2. MetaDrive version: 0.4.1.2\u001b[0m\n",":device(error): Error adding inotify watch on /dev/input: No such file or directory\n",":device(error): Error opening directory /dev/input: No such file or directory\n","\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n","\u001b[38;20m[INFO] Start Scenario Index: 1010, Num Scenarios : 1000\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 1414 Reason: out_of_road.\u001b[0m\n","Finish 100/100 simulation steps. Time elapse: 0.5460. Average FPS: 394.8160, Average number of vehicles: 3.0000\n","Total Time Elapse: 0.546, average FPS: 394.612, average number of vehicles: 3.000.\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n","Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.45.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Collecting pyglet<2.0.0\n","  Downloading pyglet-1.5.28-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyglet\n","Successfully installed pyglet-1.5.28\n","Mounted at /content/gdrive\n","/content/gdrive/MyDrive/cs260r/assignment4\n","/content/gdrive/MyDrive/cs260r/assignment4\n","agents\t\t\t\t  README.md\t\t\t  train_ppo_in_singleagent_env\n","core\t\t\t\t  step_info.py\t\t\t  train_ppo_in_singleagent_env_2\n","eval_multi_agent_performance.py   train_ppo_in_multiagent_env\t  train_ppo_in_singleagent_env.py\n","eval_single_agent_performance.py  train_ppo_in_multiagent_env.py  train_ppo_in_singleagent_env.sh\n","notebook.ipynb\t\t\t  train_ppo_in_multiagent_env.sh\n"]}],"source":["!pip install torch\n","\n","# Install MetaDrive, a lightweight driving simulator\n","!pip install git+https://github.com/metadriverse/metadrive\n","\n","# Test whether MetaDrive is properly installed. No error means the test is passed.\n","!python -m metadrive.examples.profile_metadrive --num-steps 100\n","\n","!pip install pandas scipy seaborn tabulate pyyaml\n","\n","# Update(2022-11-03): Fix pyglet compatability issue since it is updated to 2.0.0 recently.\n","!pip install \"pyglet<2.0.0\"\n","\n","import os\n","try:\n","\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","\n","    DRIVE_PATH = '/content/gdrive/MyDrive/cs260r/assignment4'\n","    DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","    if not os.path.exists(DRIVE_PYTHON_PATH):\n","      %mkdir $DRIVE_PATH\n","\n","    ## the space in `My Drive` causes some issues,\n","    ## make a symlink to avoid this\n","    SYM_PATH = '/content/gdrive/MyDrive/cs260r/assignment4'\n","    if not os.path.exists(SYM_PATH):\n","      !ln -s $DRIVE_PATH $SYM_PATH\n","\n","    running_in_colab = True\n","\n","    # We already mounted in our google drive.\n","    # Enter the foler where you put files in:\n","    %cd '/content/gdrive/MyDrive/cs260r/assignment4'\n","\n","    # Current working directory:\n","    !pwd\n","\n","    # What files are there:\n","    !ls\n","\n","\n","except ModuleNotFoundError:\n","    running_in_colab = False\n","    print(\n","        \"I guess you are running locally. If you get this message in Colab, check the files.\"\n","    )\n"]},{"cell_type":"markdown","source":["# RUN CODE"],"metadata":{"id":"wBY2Lp3935uI"}},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6XdukcJm4v_3","executionInfo":{"status":"ok","timestamp":1701576861008,"user_tz":480,"elapsed":165,"user":{"displayName":"JINWOO BAIK","userId":"01620325729908986677"}},"outputId":"587f9e6f-5978-4646-f111-c233de2483e2"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["agents\t\t\t\t  README.md\t\t\t  train_ppo_in_singleagent_env\n","core\t\t\t\t  step_info.py\t\t\t  train_ppo_in_singleagent_env_2\n","eval_multi_agent_performance.py   train_ppo_in_multiagent_env\t  train_ppo_in_singleagent_env.py\n","eval_single_agent_performance.py  train_ppo_in_multiagent_env.py  train_ppo_in_singleagent_env.sh\n","notebook.ipynb\t\t\t  train_ppo_in_multiagent_env.sh\n"]}]},{"cell_type":"code","source":["# eval\n","%%shell\n","\n","python eval_multi_agent_performance.py --num-processes 2 --num-episodes 5 --render\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"OR1JNFZB4ZW9","executionInfo":{"status":"error","timestamp":1701577736306,"user_tz":480,"elapsed":8477,"user":{"displayName":"JINWOO BAIK","userId":"01620325729908986677"}},"outputId":"e1fd01d7-e351-4dc4-816a-6ce4552013c8"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[38;20m[INFO] Environment: MultiAgentRacingEnvWithSimplifiedReward\u001b[0m\n","\u001b[38;20m[INFO] MetaDrive version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector(), dashboard: DashBoard(), main_camera: MainCamera(1200, 900)]\u001b[0m\n","\u001b[38;20m[INFO] Render Mode: onscreen\u001b[0m\n","\u001b[38;20m[INFO] Horizon (Max steps per agent): 3000\u001b[0m\n","\u001b[33;20m[WARNING] When reaching max steps, both 'terminate' and 'truncate will be True.Generally, only the `truncate` should be `True`. (base_env.py:399)\u001b[0m\n","Successfully loaded weights from /content/gdrive/MyDrive/cs260r/assignment4/agents/ppo1/checkpoint-iter25.pkl!\n","Successfully loaded weights from /content/gdrive/MyDrive/cs260r/assignment4/agents/ppo1/checkpoint-iter25.pkl!\n","Successfully loaded weights from /content/gdrive/MyDrive/cs260r/assignment4/agents/ppo/checkpoint-iter25.pkl!\n","Successfully loaded weights from /content/gdrive/MyDrive/cs260r/assignment4/agents/ppo/checkpoint-iter25.pkl!\n","==================================================\n","EVALUATING AGENTS dict_keys(['agent1', 'agent0']) IN MULTI-AGENT ENVIRONMENT.\n","==================================================\n","Start evaluation!\n","\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n",":ShowBase(warning): Unable to open 'onscreen' window.\n","Traceback (most recent call last):\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/eval_multi_agent_performance.py\", line 146, in <module>\n","    obs_dict, _ = env.reset()\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/envs/marl_envs/marl_racing_env.py\", line 384, in reset\n","    ret = super(MultiAgentRacingEnv, self).reset(seed=seed)\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/envs/base_env.py\", line 554, in reset\n","    self.lazy_init()  # it only works the first time when reset() is called to avoid the error when render\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/envs/base_env.py\", line 431, in lazy_init\n","    initialize_engine(self.config)\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/engine/engine_utils.py\", line 12, in initialize_engine\n","    cls.singleton = cls(env_global_config)\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/engine/base_engine.py\", line 55, in __init__\n","    EngineCore.__init__(self, global_config)\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/engine/core/engine_core.py\", line 181, in __init__\n","    super(EngineCore, self).__init__(windowType=self.mode)\n","  File \"/usr/local/lib/python3.10/dist-packages/direct/showbase/ShowBase.py\", line 341, in __init__\n","    self.openDefaultWindow(startDirect = False, props=props)\n","  File \"/usr/local/lib/python3.10/dist-packages/direct/showbase/ShowBase.py\", line 1026, in openDefaultWindow\n","    self.openMainWindow(*args, **kw)\n","  File \"/usr/local/lib/python3.10/dist-packages/direct/showbase/ShowBase.py\", line 1061, in openMainWindow\n","    self.openWindow(*args, **kw)\n","  File \"/usr/local/lib/python3.10/dist-packages/direct/showbase/ShowBase.py\", line 806, in openWindow\n","    raise Exception('Could not open window.')\n","Exception: Could not open window.\n"]},{"output_type":"error","ename":"CalledProcessError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-79f6ad68b4a5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\npython eval_multi_agent_performance.py --num-processes 2 --num-episodes 5 --render\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m       raise subprocess.CalledProcessError(\n\u001b[0m\u001b[1;32m    138\u001b[0m           \u001b[0mreturncode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m       )\n","\u001b[0;31mCalledProcessError\u001b[0m: Command '\npython eval_multi_agent_performance.py --num-processes 2 --num-episodes 5 --render\n' returned non-zero exit status 1."]}]},{"cell_type":"code","source":["# TRAIN AGENT FROM SCRATCH\n","%%shell\n","# python train_ppo_in_singleagent_env.py \\\n","# --log-dir train_ppo_in_singleagent_env \\\n","# --num-processes 10 \\\n","# --num-steps 4_000 \\\n","# --max-steps 1_500_000\n","\n","# python train_ppo_in_singleagent_env.py \\\n","# --log-dir train_ppo_in_singleagent_env_2 \\\n","# --num-processes 10 \\\n","# --num-steps 4_000 \\\n","# --pretrained-model-suffix final \\\n","# --pretrained-model-log-dir train_ppo_in_singleagent_env/ppo \\\n","# --max-steps 2_000_000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GQAB43u635Ng","outputId":"c4f32971-b046-4281-81d4-68211d2ffefd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[38;20m[INFO] Environment: SingleAgentRacingEnv\u001b[0m\n","\u001b[38;20m[INFO] MetaDrive version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()]\u001b[0m\n","\u001b[38;20m[INFO] Render Mode: none\u001b[0m\n","\u001b[38;20m[INFO] Horizon (Max steps per agent): 3000\u001b[0m\n","\u001b[33;20m[WARNING] When reaching max steps, both 'terminate' and 'truncate will be True.Generally, only the `truncate` should be `True`. (base_env.py:382)\u001b[0m\n","Successfully loaded weights from /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env/ppo/checkpoint-final.pkl!\n","Successfully loaded pretrained model at train_ppo_in_singleagent_env/ppo with suffix final.\n","Start training!\n","\u001b[38;20m[INFO] Environment: SingleAgentRacingEnv\u001b[0m\n","\u001b[38;20m[INFO] MetaDrive version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()]\u001b[0m\n","\u001b[38;20m[INFO] Render Mode: none\u001b[0m\n","\u001b[38;20m[INFO] Horizon (Max steps per agent): 3000\u001b[0m\n","\u001b[33;20m[WARNING] When reaching max steps, both 'terminate' and 'truncate will be True.Generally, only the `truncate` should be `True`. (base_env.py:382)\u001b[0m\n","\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Environment: SingleAgentRacingEnv\u001b[0m\n","\u001b[38;20m[INFO] MetaDrive version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()]\u001b[0m\n","\u001b[38;20m[INFO] Render Mode: none\u001b[0m\n","\u001b[38;20m[INFO] Horizon (Max steps per agent): 3000\u001b[0m\n","\u001b[33;20m[WARNING] When reaching max steps, both 'terminate' and 'truncate will be True.Generally, only the `truncate` should be `True`. (base_env.py:382)\u001b[0m\n","\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n",":device(error): Error adding inotify watch on /dev/input: No such file or directory\n",":device(error): Error opening directory /dev/input: No such file or directory\n","\u001b[38;20m[INFO] Environment: SingleAgentRacingEnv\u001b[0m\n","\u001b[38;20m[INFO] MetaDrive version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Environment: SingleAgentRacingEnv\u001b[0m\n","\u001b[38;20m[INFO] MetaDrive version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()]\u001b[0m\n","\u001b[38;20m[INFO] Render Mode: none\u001b[0m\n","\u001b[38;20m[INFO] Horizon (Max steps per agent): 3000\u001b[0m\n","\u001b[33;20m[WARNING] When reaching max steps, both 'terminate' and 'truncate will be True.Generally, only the `truncate` should be `True`. (base_env.py:382)\u001b[0m\n","\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()]\u001b[0m\n","\u001b[38;20m[INFO] Render Mode: none\u001b[0m\n","\u001b[38;20m[INFO] Horizon (Max steps per agent): 3000\u001b[0m\n","\u001b[33;20m[WARNING] When reaching max steps, both 'terminate' and 'truncate will be True.Generally, only the `truncate` should be `True`. (base_env.py:382)\u001b[0m\n",":device(error): Error adding inotify watch on /dev/input: No such file or directory\n",":device(error): Error opening directory /dev/input: No such file or directory\n","\u001b[38;20m[INFO] Environment: SingleAgentRacingEnv\u001b[0m\n","\u001b[38;20m[INFO] Environment: SingleAgentRacingEnv\u001b[0m\n","\u001b[38;20m[INFO] MetaDrive version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] MetaDrive version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()]\u001b[0m\n","\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()]\u001b[0m\n","\u001b[38;20m[INFO] Render Mode: none\u001b[0m\n","\u001b[38;20m[INFO] Horizon (Max steps per agent): 3000\u001b[0m\n","\u001b[33;20m[WARNING] When reaching max steps, both 'terminate' and 'truncate will be True.Generally, only the `truncate` should be `True`. (base_env.py:382)\u001b[0m\n","\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Render Mode: none\u001b[0m\n","\u001b[38;20m[INFO] Horizon (Max steps per agent): 3000\u001b[0m\n","\u001b[33;20m[WARNING] When reaching max steps, both 'terminate' and 'truncate will be True.Generally, only the `truncate` should be `True`. (base_env.py:382)\u001b[0m\n","\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Environment: SingleAgentRacingEnv\u001b[0m\n","\u001b[38;20m[INFO] MetaDrive version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()]\u001b[0m\n","\u001b[38;20m[INFO] Render Mode: none\u001b[0m\n","\u001b[38;20m[INFO] Horizon (Max steps per agent): 3000\u001b[0m\n","\u001b[33;20m[WARNING] When reaching max steps, both 'terminate' and 'truncate will be True.Generally, only the `truncate` should be `True`. (base_env.py:382)\u001b[0m\n","\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n","\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n",":device\u001b[38;20m[INFO] Start Scenario Index: 0, Num Scenarios : 1\u001b[0m\n","(error): Error adding inotify watch on /dev/input: No such file or directory\n",":device(error): Error opening directory /dev/input: No such file or directory\n",":device(error): Error adding inotify watch on /dev/input: No such file or directory\n",":device(error): Error opening directory /dev/input: No such file or directory\n",":device:device(error): Error adding inotify watch on /dev/input: No such file or directory\n",":device(error): Error opening directory /dev/input: No such file or directory\n","(error): Error adding inotify watch on /dev/input: No such file or directory\n",":device(error): Error opening directory /dev/input: No such file or directory\n","\u001b[38;20m[INFO] Start Scenario Index: 0, Num Scenarios : 1\u001b[0m\n","\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n","\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n",":device(error): Error adding inotify watch on /dev/input: No such file or directory\n",":device(error): Error opening directory /dev/input: No such file or directory\n","\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n","\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n","\u001b[38;20m[INFO] Start Scenario Index: 0, Num Scenarios : 1\u001b[0m\n","\u001b[38;20m[INFO] Start Scenario Index: 0, Num Scenarios : 1\u001b[0m\n","\u001b[38;20m[INFO] Start Scenario Index: 0, Num Scenarios : 1\u001b[0m\n","\u001b[38;20m[INFO] Start Scenario Index: 0, Num Scenarios : 1\u001b[0m\n","\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n","\u001b[38;20m[INFO] Environment: SingleAgentRacingEnv\u001b[0m\n","\u001b[38;20m[INFO] MetaDrive version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()]\u001b[0m\n","\u001b[38;20m[INFO] Render Mode: none\u001b[0m\n","\u001b[38;20m[INFO] Horizon (Max steps per agent): 3000\u001b[0m\n","\u001b[33;20m[WARNING] When reaching max steps, both 'terminate' and 'truncate will be True.Generally, only the `truncate` should be `True`. (base_env.py:382)\u001b[0m\n","\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Start Scenario Index: 0, Num Scenarios : 1\u001b[0m\n","\u001b[38;20m[INFO] Environment: SingleAgentRacingEnv\u001b[0m\n","\u001b[38;20m[INFO] MetaDrive version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()]\u001b[0m\n","\u001b[38;20m[INFO] Render Mode: none\u001b[0m\n","\u001b[38;20m[INFO] Horizon (Max steps per agent): 3000\u001b[0m\n","\u001b[33;20m[WARNING] When reaching max steps, both 'terminate' and 'truncate will be True.Generally, only the `truncate` should be `True`. (base_env.py:382)\u001b[0m\n","\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n",":device(error): Error adding inotify watch on /dev/input: No such file or directory\n",":device(error): Error opening directory /dev/input: No such file or directory\n","\u001b[38;20m[INFO] Environment: SingleAgentRacingEnv\u001b[0m\n","\u001b[38;20m[INFO] MetaDrive version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Sensors: [lidar: Lidar(), side_detector: SideDetector(), lane_line_detector: LaneLineDetector()]\u001b[0m\n","\u001b[38;20m[INFO] Render Mode: none\u001b[0m\n","\u001b[38;20m[INFO] Horizon (Max steps per agent): 3000\u001b[0m\n","\u001b[33;20m[WARNING] When reaching max steps, both 'terminate' and 'truncate will be True.Generally, only the `truncate` should be `True`. (base_env.py:382)\u001b[0m\n","\u001b[38;20m[INFO] Assets version: 0.4.1.2\u001b[0m\n","\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n",":device(error): Error adding inotify watch on /dev/input: No such file or directory\n",":device(error): Error opening directory /dev/input: No such file or directory\n",":device(error): Error adding inotify watch on /dev/input: No such file or directory\n",":device(error): Error opening directory /dev/input: No such file or directory\n","\u001b[38;20m[INFO] Start Scenario Index: 0, Num Scenarios : 1\u001b[0m\n","\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n","\u001b[38;20m[INFO] Known Pipes: glxGraphicsPipe\u001b[0m\n","\u001b[38;20m[INFO] Start Scenario Index: 0, Num Scenarios : 1\u001b[0m\n","\u001b[38;20m[INFO] Start Scenario Index: 0, Num Scenarios : 1\u001b[0m\n","  0% 0/2000000 [00:00<?, ?it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 0 =====:\n","  adv_mean: 2.9405808448791504\n","  crash_sidewalk_rate: 0.0128\n","  crash_vehicle_rate: 0.0\n","  entropy: 1.2864123958043563\n","  episode_length: 1830.95\n","  episode_reward: 1196.7846219946944\n","  frame_per_second: 270\n","  grad_norm: 1455.1998464144194\n","  idle_rate: 3.3333333333333335e-05\n","  iteration: 0\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.00043864764738827945\n","  ratio: 0.9997747401396434\n","  speed_km_h: 40.21184236646803\n","  success_rate: 0.95\n","  total_episodes: 20\n","  total_loss: 2041.738072082324\n","  total_steps: 40000\n","  total_time: 147.94547605514526\n","  value_loss: 4083.4770271007833\n","\n","Trainer is saved at </content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo/checkpoint-iter0.pkl>. Progress is saved at </content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo/progress.csv>.\n","  2% 40000/2000000 [02:30<1:48:09, 302.02it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 1 =====:\n","  adv_mean: 5.241514682769775\n","  crash_sidewalk_rate: 0.009333333333333334\n","  crash_vehicle_rate: 0.0\n","  entropy: 1.2154739964466827\n","  episode_length: 1801.325\n","  episode_reward: 1624.398934036727\n","  frame_per_second: 287\n","  grad_norm: 1358.2563629737267\n","  idle_rate: 0.0\n","  iteration: 1\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0007084268678087168\n","  ratio: 1.0005326471267602\n","  speed_km_h: 43.14987112874155\n","  success_rate: 0.975\n","  total_episodes: 40\n","  total_loss: 1270.0743372697098\n","  total_steps: 80000\n","  total_time: 278.1035120487213\n","  value_loss: 2540.15010402386\n","\n","  4% 80000/2000000 [04:22<1:44:51, 305.16it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","  4% 80000/2000000 [04:40<1:44:51, 305.16it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 2 =====:\n","  adv_mean: 0.910608172416687\n","  crash_sidewalk_rate: 0.015533333333333333\n","  crash_vehicle_rate: 0.0\n","  entropy: 1.16714839163499\n","  episode_length: 1754.4754098360656\n","  episode_reward: 1188.4391958562394\n","  frame_per_second: 292\n","  grad_norm: 2197.1184053078678\n","  idle_rate: 3.3333333333333335e-05\n","  iteration: 2\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0003246093291240052\n","  ratio: 0.9999297290276259\n","  speed_km_h: 45.00159431149422\n","  success_rate: 0.9344262295081968\n","  total_episodes: 61\n","  total_loss: 1961.2264621441182\n","  total_steps: 120000\n","  total_time: 410.8751459121704\n","  value_loss: 3922.453568815574\n","\n","  6% 120000/2000000 [06:35<1:43:17, 303.36it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","  6% 120000/2000000 [06:50<1:43:17, 303.36it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 3 =====:\n","  adv_mean: 7.582755088806152\n","  crash_sidewalk_rate: 0.003266666666666667\n","  crash_vehicle_rate: 0.0\n","  entropy: 1.1199008768185592\n","  episode_length: 1724.4888888888888\n","  episode_reward: 1456.6213072218288\n","  frame_per_second: 293\n","  grad_norm: 332.8973325338119\n","  idle_rate: 0.0\n","  iteration: 3\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.002432900466597997\n","  ratio: 1.0007046593305393\n","  speed_km_h: 47.003508618140536\n","  success_rate: 0.9555555555555556\n","  total_episodes: 90\n","  total_loss: 211.6862154936179\n","  total_steps: 160000\n","  total_time: 545.3009464740753\n","  value_loss: 423.37729736719376\n","\n","  8% 160000/2000000 [08:49<1:41:51, 301.05it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","  8% 160000/2000000 [09:00<1:41:51, 301.05it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\n"," ===== ppo Training Iteration 4 =====:\n","  adv_mean: 3.808194875717163\n","  crash_sidewalk_rate: 0.009466666666666667\n","  crash_vehicle_rate: 0.0\n","  entropy: 1.0560082335502674\n","  episode_length: 1671.88\n","  episode_reward: 1387.529765287193\n","  frame_per_second: 295\n","  grad_norm: 1306.0480395879501\n","  idle_rate: 3.3333333333333335e-05\n","  iteration: 4\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0004986531535485902\n","  ratio: 0.9996488146292858\n","  speed_km_h: 50.42886309480167\n","  success_rate: 0.94\n","  total_episodes: 113\n","  total_loss: 964.3017358486469\n","  total_steps: 200000\n","  total_time: 676.8039193153381\n","  value_loss: 1928.6044682625013\n","\n"," 10% 200000/2000000 [11:01<1:39:17, 302.16it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 10% 200000/2000000 [11:20<1:39:17, 302.16it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 5 =====:\n","  adv_mean: 5.2573442459106445\n","  crash_sidewalk_rate: 0.007333333333333333\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.9864699125671998\n","  episode_length: 1593.4\n","  episode_reward: 1682.23740352617\n","  frame_per_second: 294\n","  grad_norm: 1194.7782314838507\n","  idle_rate: 0.0\n","  iteration: 5\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0008727914710433629\n","  ratio: 0.99998241907511\n","  speed_km_h: 52.440957753787295\n","  success_rate: 0.96\n","  total_episodes: 141\n","  total_loss: 779.0147115316147\n","  total_steps: 240000\n","  total_time: 813.9820885658264\n","  value_loss: 1558.031169695732\n","\n","Trainer is saved at </content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo/checkpoint-iter5.pkl>. Progress is saved at </content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo/progress.csv>.\n"," 12% 240000/2000000 [13:18<1:38:16, 298.47it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 12% 240000/2000000 [13:30<1:38:16, 298.47it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 6 =====:\n","  adv_mean: 3.639956474304199\n","  crash_sidewalk_rate: 0.005366666666666666\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.9409232242749288\n","  episode_length: 1489.87\n","  episode_reward: 1691.5960194243855\n","  frame_per_second: 294\n","  grad_norm: 1071.1599208098191\n","  idle_rate: 0.0\n","  iteration: 6\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0005048612126698479\n","  ratio: 0.9999764165817163\n","  speed_km_h: 55.25601404737275\n","  success_rate: 0.95\n","  total_episodes: 171\n","  total_loss: 800.7161083808312\n","  total_steps: 280000\n","  total_time: 951.8489842414856\n","  value_loss: 1601.433232820951\n","\n"," 14% 280000/2000000 [15:36<1:36:56, 295.71it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 14% 280000/2000000 [15:50<1:36:56, 295.71it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 7 =====:\n","  adv_mean: -0.6599026322364807\n","  crash_sidewalk_rate: 0.014533333333333334\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.8956720563845757\n","  episode_length: 1421.0\n","  episode_reward: 1594.8432850858233\n","  frame_per_second: 294\n","  grad_norm: 2155.4527214148106\n","  idle_rate: 3.3333333333333335e-05\n","  iteration: 7\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0005119015976117971\n","  ratio: 0.9999868255777237\n","  speed_km_h: 56.13676479101722\n","  success_rate: 0.95\n","  total_episodes: 201\n","  total_loss: 1418.7959159851075\n","  total_steps: 320000\n","  total_time: 1087.393503665924\n","  value_loss: 2837.5928618602265\n","\n"," 16% 320000/2000000 [17:51<1:34:44, 295.52it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 16% 320000/2000000 [18:10<1:34:44, 295.52it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 8 =====:\n","  adv_mean: -4.288937091827393\n","  crash_sidewalk_rate: 0.016866666666666665\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.9052280762638801\n","  episode_length: 1360.76\n","  episode_reward: 1260.3514332483828\n","  frame_per_second: 293\n","  grad_norm: 3068.187670037685\n","  idle_rate: 6.666666666666667e-05\n","  iteration: 8\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0004315957815075914\n","  ratio: 0.999918016180014\n","  speed_km_h: 56.573354409958405\n","  success_rate: 0.92\n","  total_episodes: 232\n","  total_loss: 3179.295089526054\n","  total_steps: 360000\n","  total_time: 1225.206618309021\n","  value_loss: 6358.591055689102\n","\n"," 18% 360000/2000000 [20:20<1:33:01, 293.85it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 9 =====:\n","  adv_mean: -0.9781485199928284\n","  crash_sidewalk_rate: 0.0068\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.9021390063258318\n","  episode_length: 1339.41\n","  episode_reward: 1195.9776857973948\n","  frame_per_second: 293\n","  grad_norm: 2936.925656421368\n","  idle_rate: 0.0\n","  iteration: 9\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.00018329458081354507\n","  ratio: 1.0000148705565013\n","  speed_km_h: 57.289749339492886\n","  success_rate: 0.92\n","  total_episodes: 261\n","  total_loss: 2384.037435091459\n","  total_steps: 400000\n","  total_time: 1362.485853433609\n","  value_loss: 4768.075229625213\n","\n"," 20% 400000/2000000 [22:26<1:30:59, 293.08it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 20% 400000/2000000 [22:40<1:30:59, 293.08it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 10 =====:\n","  adv_mean: -1.8715986013412476\n","  crash_sidewalk_rate: 0.02033333333333333\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.861859561693974\n","  episode_length: 1338.26\n","  episode_reward: 1109.4778729828586\n","  frame_per_second: 294\n","  grad_norm: 3322.5426016391852\n","  idle_rate: 3.3333333333333335e-05\n","  iteration: 10\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.000584080831732792\n","  ratio: 0.9998863774996537\n","  speed_km_h: 56.57008575759159\n","  success_rate: 0.91\n","  total_episodes: 291\n","  total_loss: 2585.9829985202887\n","  total_steps: 440000\n","  total_time: 1493.405377626419\n","  value_loss: 5171.967166998447\n","\n","Trainer is saved at </content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo/checkpoint-iter10.pkl>. Progress is saved at </content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo/progress.csv>.\n"," 22% 440000/2000000 [24:37<1:27:36, 296.77it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 22% 440000/2000000 [24:50<1:27:36, 296.77it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 11 =====:\n","  adv_mean: -4.192722320556641\n","  crash_sidewalk_rate: 0.008166666666666666\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.8400493984039014\n","  episode_length: 1335.32\n","  episode_reward: 983.9047028967599\n","  frame_per_second: 295\n","  grad_norm: 4107.005733509553\n","  idle_rate: 3.3333333333333335e-05\n","  iteration: 11\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.00046907146819508995\n","  ratio: 0.999996109192188\n","  speed_km_h: 58.832478701971866\n","  success_rate: 0.9\n","  total_episodes: 320\n","  total_loss: 3737.0876775497045\n","  total_steps: 480000\n","  total_time: 1622.103759765625\n","  value_loss: 7474.176295040816\n","\n"," 24% 480000/2000000 [26:46<1:24:11, 300.92it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 24% 480000/2000000 [27:00<1:24:11, 300.92it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 12 =====:\n","  adv_mean: 1.9230784177780151\n","  crash_sidewalk_rate: 0.0101\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.7970307936653113\n","  episode_length: 1331.14\n","  episode_reward: 1272.0468481124983\n","  frame_per_second: 296\n","  grad_norm: 2069.2376086601844\n","  idle_rate: 0.0\n","  iteration: 12\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0010812893393449484\n","  ratio: 1.0001035900834279\n","  speed_km_h: 57.82941495981181\n","  success_rate: 0.91\n","  total_episodes: 350\n","  total_loss: 2065.126299334795\n","  total_steps: 520000\n","  total_time: 1752.809849023819\n","  value_loss: 4130.254769877898\n","\n"," 26% 520000/2000000 [28:57<1:21:33, 302.45it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 26% 520000/2000000 [29:10<1:21:33, 302.45it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 13 =====:\n","  adv_mean: 1.3615869283676147\n","  crash_sidewalk_rate: 0.0086\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.7634622930334165\n","  episode_length: 1291.85\n","  episode_reward: 1400.5150799508344\n","  frame_per_second: 296\n","  grad_norm: 1408.9331468924497\n","  idle_rate: 3.3333333333333335e-05\n","  iteration: 13\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.00015850649051105556\n","  ratio: 1.0000735683318895\n","  speed_km_h: 59.29984993734263\n","  success_rate: 0.9\n","  total_episodes: 385\n","  total_loss: 1619.6996117127248\n","  total_steps: 560000\n","  total_time: 1888.852338552475\n","  value_loss: 3239.399542236328\n","\n"," 28% 560000/2000000 [31:13<1:20:02, 299.86it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 28% 560000/2000000 [31:30<1:20:02, 299.86it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 14 =====:\n","  adv_mean: -4.24213171005249\n","  crash_sidewalk_rate: 0.021833333333333333\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.7400376442533273\n","  episode_length: 1287.53\n","  episode_reward: 1374.2367431428356\n","  frame_per_second: 297\n","  grad_norm: 3275.879893796872\n","  idle_rate: 3.3333333333333335e-05\n","  iteration: 14\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0009298970030310254\n","  ratio: 1.0000510208499738\n","  speed_km_h: 58.18462539702972\n","  success_rate: 0.89\n","  total_episodes: 414\n","  total_loss: 2232.549810018295\n","  total_steps: 600000\n","  total_time: 2019.4339623451233\n","  value_loss: 4465.10147750072\n","\n"," 30% 600000/2000000 [33:23<1:17:19, 301.78it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 30% 600000/2000000 [33:40<1:17:19, 301.78it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 15 =====:\n","  adv_mean: -1.8584935665130615\n","  crash_sidewalk_rate: 0.019166666666666665\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.7291156883423145\n","  episode_length: 1287.31\n","  episode_reward: 1218.984683533891\n","  frame_per_second: 296\n","  grad_norm: 3465.36016790928\n","  idle_rate: 6.666666666666667e-05\n","  iteration: 15\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.00016869692048296714\n","  ratio: 1.0000448926519125\n","  speed_km_h: 59.03059407188003\n","  success_rate: 0.88\n","  total_episodes: 445\n","  total_loss: 2900.459798832429\n","  total_steps: 640000\n","  total_time: 2158.182867050171\n","  value_loss: 5800.919929132706\n","\n","Trainer is saved at </content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo/checkpoint-iter15.pkl>. Progress is saved at </content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo/progress.csv>.\n"," 32% 640000/2000000 [35:42<1:16:10, 297.56it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 32% 640000/2000000 [36:00<1:16:10, 297.56it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 16 =====:\n","  adv_mean: 2.815222978591919\n","  crash_sidewalk_rate: 0.004933333333333333\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.7143405290368275\n","  episode_length: 1298.23\n","  episode_reward: 1387.5280290713092\n","  frame_per_second: 296\n","  grad_norm: 687.2965202673888\n","  idle_rate: 0.0\n","  iteration: 16\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0005356816700301491\n","  ratio: 1.0000299934011239\n","  speed_km_h: 59.147054286709476\n","  success_rate: 0.9\n","  total_episodes: 475\n","  total_loss: 426.25195456284746\n","  total_steps: 680000\n","  total_time: 2293.1717965602875\n","  value_loss: 852.5049805274376\n","\n"," 34% 680000/2000000 [37:57<1:14:01, 297.21it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 34% 680000/2000000 [38:10<1:14:01, 297.21it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 17 =====:\n","  adv_mean: 2.707282304763794\n","  crash_sidewalk_rate: 0.005566666666666667\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.7197414019933114\n","  episode_length: 1290.65\n","  episode_reward: 1623.1954765907972\n","  frame_per_second: 295\n","  grad_norm: 1352.005413916172\n","  idle_rate: 0.0\n","  iteration: 17\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0005558342093792863\n","  ratio: 0.9998554883094934\n","  speed_km_h: 60.44117276827168\n","  success_rate: 0.9\n","  total_episodes: 506\n","  total_loss: 968.4731052300868\n","  total_steps: 720000\n","  total_time: 2432.546216249466\n","  value_loss: 1936.9473275991586\n","\n"," 36% 720000/2000000 [40:17<1:12:32, 294.07it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 36% 720000/2000000 [40:30<1:12:32, 294.07it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 18 =====:\n","  adv_mean: -9.847542762756348\n","  crash_sidewalk_rate: 0.0288\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.7096178583991833\n","  episode_length: 1285.74\n","  episode_reward: 1472.2559448598936\n","  frame_per_second: 295\n","  grad_norm: 3820.2304474952894\n","  idle_rate: 6.666666666666667e-05\n","  iteration: 18\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0004917788101384082\n","  ratio: 0.9996530619951395\n","  speed_km_h: 58.749864548826686\n","  success_rate: 0.92\n","  total_episodes: 538\n","  total_loss: 2522.1306840945513\n","  total_steps: 760000\n","  total_time: 2572.450144290924\n","  value_loss: 5044.2623560783195\n","\n"," 38% 760000/2000000 [42:36<1:10:52, 291.57it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 38% 760000/2000000 [42:50<1:10:52, 291.57it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 19 =====:\n","  adv_mean: 1.93211829662323\n","  crash_sidewalk_rate: 0.006566666666666667\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.6455041223229506\n","  episode_length: 1277.03\n","  episode_reward: 1442.8376291731283\n","  frame_per_second: 295\n","  grad_norm: 735.8996478863252\n","  idle_rate: 0.0\n","  iteration: 19\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0001458518365278649\n","  ratio: 0.9993810044649319\n","  speed_km_h: 60.6266402325859\n","  success_rate: 0.92\n","  total_episodes: 569\n","  total_loss: 570.0108975728352\n","  total_steps: 800000\n","  total_time: 2710.0024251937866\n","  value_loss: 1140.0220838889097\n","\n"," 40% 800000/2000000 [44:54<1:08:39, 291.33it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 40% 800000/2000000 [45:10<1:08:39, 291.33it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 20 =====:\n","  adv_mean: 2.201646327972412\n","  crash_sidewalk_rate: 0.0054\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.6522168648930696\n","  episode_length: 1277.09\n","  episode_reward: 1470.5051816366001\n","  frame_per_second: 294\n","  grad_norm: 712.7452355287014\n","  idle_rate: 0.0\n","  iteration: 20\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0006742482757171951\n","  ratio: 1.0000575816402069\n","  speed_km_h: 60.44698889891451\n","  success_rate: 0.95\n","  total_episodes: 602\n","  total_loss: 453.37262960580676\n","  total_steps: 840000\n","  total_time: 2848.3451721668243\n","  value_loss: 906.746607139783\n","\n","Trainer is saved at </content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo/checkpoint-iter20.pkl>. Progress is saved at </content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo/progress.csv>.\n"," 42% 840000/2000000 [47:12<1:06:30, 290.65it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 42% 840000/2000000 [47:30<1:06:30, 290.65it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 21 =====:\n","  adv_mean: 2.0044264793395996\n","  crash_sidewalk_rate: 0.004966666666666667\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.6016452136330116\n","  episode_length: 1255.03\n","  episode_reward: 1720.8941416684495\n","  frame_per_second: 294\n","  grad_norm: 482.72267610843363\n","  idle_rate: 0.0\n","  iteration: 21\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.00037177243487885556\n","  ratio: 1.0001924427273945\n","  speed_km_h: 60.84907210453714\n","  success_rate: 0.95\n","  total_episodes: 633\n","  total_loss: 302.74833486019037\n","  total_steps: 880000\n","  total_time: 2986.2605991363525\n","  value_loss: 605.4974132440029\n","\n"," 44% 880000/2000000 [49:30<1:04:15, 290.49it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 44% 880000/2000000 [49:50<1:04:15, 290.49it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 22 =====:\n","  adv_mean: -3.370478868484497\n","  crash_sidewalk_rate: 0.015133333333333334\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.5530319432417552\n","  episode_length: 1258.15\n","  episode_reward: 1732.830491693597\n","  frame_per_second: 294\n","  grad_norm: 1675.6241748712002\n","  idle_rate: 6.666666666666667e-05\n","  iteration: 22\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0008323057488992046\n","  ratio: 0.9999574004075467\n","  speed_km_h: 61.440438061113646\n","  success_rate: 0.96\n","  total_episodes: 665\n","  total_loss: 939.3044138639401\n","  total_steps: 920000\n","  total_time: 3123.004928588867\n","  value_loss: 1878.6104980077498\n","\n"," 46% 920000/2000000 [51:47<1:01:50, 291.09it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 46% 920000/2000000 [52:00<1:01:50, 291.09it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 23 =====:\n","  adv_mean: 1.1846725940704346\n","  crash_sidewalk_rate: 0.006066666666666666\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.54129121542359\n","  episode_length: 1248.41\n","  episode_reward: 1718.2879497957742\n","  frame_per_second: 294\n","  grad_norm: 552.9283977190654\n","  idle_rate: 0.0\n","  iteration: 23\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.00020327444922608825\n","  ratio: 1.0001950079813982\n","  speed_km_h: 62.259873261712876\n","  success_rate: 0.96\n","  total_episodes: 697\n","  total_loss: 346.86656765081943\n","  total_steps: 960000\n","  total_time: 3262.8799386024475\n","  value_loss: 693.7335411952092\n","\n"," 48% 960000/2000000 [54:07<59:51, 289.54it/s]  \u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 48% 960000/2000000 [54:20<59:51, 289.54it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 24 =====:\n","  adv_mean: 0.28930729627609253\n","  crash_sidewalk_rate: 0.0057\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.4959646822168277\n","  episode_length: 1244.16\n","  episode_reward: 1703.016267360498\n","  frame_per_second: 293\n","  grad_norm: 606.6855012600239\n","  idle_rate: 3.3333333333333335e-05\n","  iteration: 24\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.00031328050879379495\n","  ratio: 0.9998154997825622\n","  speed_km_h: 61.25046373940454\n","  success_rate: 0.95\n","  total_episodes: 729\n","  total_loss: 438.76782254928196\n","  total_steps: 1000000\n","  total_time: 3402.7834753990173\n","  value_loss: 877.5362715011988\n","\n"," 50% 1000000/2000000 [56:27<57:46, 288.44it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 50% 1000000/2000000 [56:40<57:46, 288.44it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\n"," ===== ppo Training Iteration 25 =====:\n","  adv_mean: 1.308272123336792\n","  crash_sidewalk_rate: 0.006\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.44147423098866756\n","  episode_length: 1226.22\n","  episode_reward: 1825.9605154100427\n","  frame_per_second: 293\n","  grad_norm: 624.9086141268413\n","  idle_rate: 3.3333333333333335e-05\n","  iteration: 25\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0005860282754441963\n","  ratio: 1.0002403408671037\n","  speed_km_h: 61.948757396916236\n","  success_rate: 0.94\n","  total_episodes: 764\n","  total_loss: 349.6479869793623\n","  total_steps: 1040000\n","  total_time: 3545.654224872589\n","  value_loss: 699.2971488170135\n","\n","Trainer is saved at </content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo/checkpoint-iter25.pkl>. Progress is saved at </content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo/progress.csv>.\n"," 52% 1040000/2000000 [58:50<55:58, 285.82it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 52% 1040000/2000000 [59:00<55:58, 285.82it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 26 =====:\n","  adv_mean: 0.812833309173584\n","  crash_sidewalk_rate: 0.006266666666666667\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.41381039443688516\n","  episode_length: 1216.19\n","  episode_reward: 1817.2291153932124\n","  frame_per_second: 292\n","  grad_norm: 319.59252459452705\n","  idle_rate: 3.3333333333333335e-05\n","  iteration: 26\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.00011051754819420285\n","  ratio: 0.9998776033520699\n","  speed_km_h: 63.376349456489706\n","  success_rate: 0.94\n","  total_episodes: 797\n","  total_loss: 245.61304541856813\n","  total_steps: 1080000\n","  total_time: 3686.429966211319\n","  value_loss: 491.22631242214106\n","\n"," 54% 1080000/2000000 [1:01:10<53:44, 285.34it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 54% 1080000/2000000 [1:01:30<53:44, 285.34it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 27 =====:\n","  adv_mean: -8.60833740234375\n","  crash_sidewalk_rate: 0.023433333333333334\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.4082944269363697\n","  episode_length: 1205.16\n","  episode_reward: 1547.6377453531522\n","  frame_per_second: 292\n","  grad_norm: 2401.5774985288963\n","  idle_rate: 0.0001\n","  iteration: 27\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.00024889372378731\n","  ratio: 0.9999305636836933\n","  speed_km_h: 63.14077335765274\n","  success_rate: 0.92\n","  total_episodes: 829\n","  total_loss: 2051.415719849024\n","  total_steps: 1120000\n","  total_time: 3825.36558508873\n","  value_loss: 4102.831939892892\n","\n"," 56% 1120000/2000000 [1:03:29<51:15, 286.11it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 56% 1120000/2000000 [1:03:40<51:15, 286.11it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: IDLE.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 28 =====:\n","  adv_mean: -4.6762309074401855\n","  crash_sidewalk_rate: 0.014966666666666666\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.3807354044264708\n","  episode_length: 1200.52\n","  episode_reward: 1399.2923665623787\n","  frame_per_second: 292\n","  grad_norm: 1259.2570944272554\n","  idle_rate: 6.666666666666667e-05\n","  iteration: 28\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0014236847652146258\n","  ratio: 0.9998576678908788\n","  speed_km_h: 62.94201613905664\n","  success_rate: 0.91\n","  total_episodes: 863\n","  total_loss: 2367.569158974672\n","  total_steps: 1160000\n","  total_time: 3968.4948346614838\n","  value_loss: 4735.141158568554\n","\n"," 58% 1160000/2000000 [1:05:52<49:16, 284.08it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n"," 58% 1160000/2000000 [1:06:10<49:16, 284.08it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: out_of_road.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\n"," ===== ppo Training Iteration 29 =====:\n","  adv_mean: 1.8986159563064575\n","  crash_sidewalk_rate: 0.0062\n","  crash_vehicle_rate: 0.0\n","  entropy: 0.38388089394340147\n","  episode_length: 1193.54\n","  episode_reward: 1395.126282576122\n","  frame_per_second: 107\n","  grad_norm: 659.9273994690333\n","  idle_rate: 0.0\n","  iteration: 29\n","  log_dir: /content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env_2/ppo\n","  max_step_rate: 0.0\n","  policy_loss: -0.0007244296968938449\n","  ratio: 1.0001848153196848\n","  speed_km_h: 63.330819497454286\n","  success_rate: 0.88\n","  total_episodes: 897\n","  total_loss: 400.0083638020051\n","  total_steps: 1200000\n","  total_time: 11170.09449505806\n","  value_loss: 800.0181750370906\n","\n"," 60% 1200000/2000000 [3:05:54<12:33:01, 17.71it/s]\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","\u001b[38;20m[INFO] Episode ended! Scenario Index: 0 Reason: arrive_dest.\u001b[0m\n","Process ForkServerProcess-9:\n","Process ForkServerProcess-10:\n","Process ForkServerProcess-7:\n","Process ForkServerProcess-8:\n","Process ForkServerProcess-6:\n","Process ForkServerProcess-5:\n","Process ForkServerProcess-3:\n","Process ForkServerProcess-1:\n","Process ForkServerProcess-2:\n"," 60% 1200000/2000000 [3:18:23<2:12:15, 100.81it/s]\n","Error in sys.excepthook:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/linecache.py\", line 72, in checkcache\n","    stat = os.stat(fullname)\n","KeyboardInterrupt\n","\n","Original exception was:\n","Traceback (most recent call last):\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env.py\", line 291, in <module>\n","Process ForkServerProcess-4:\n","    total_steps, episode_rewards = step_envs(\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/core/utils.py\", line 40, in step_envs\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/core/envs.py\", line 508, in _worker\n","    cmd, data = remote.recv()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n","    buf = self._recv_bytes()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n","    buf = self._recv(4)\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n","    chunk = read(handle, remaining)\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","KeyboardInterrupt\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/core/envs.py\", line 508, in _worker\n","    cmd, data = remote.recv()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n","    buf = self._recv_bytes()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n","    buf = self._recv(4)\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n","    chunk = read(handle, remaining)\n","KeyboardInterrupt\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/core/envs.py\", line 508, in _worker\n","    cmd, data = remote.recv()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n","    buf = self._recv_bytes()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n","    buf = self._recv(4)\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n","    chunk = read(handle, remaining)\n","KeyboardInterrupt\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/core/envs.py\", line 508, in _worker\n","    cmd, data = remote.recv()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n","    buf = self._recv_bytes()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n","    buf = self._recv(4)\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n","    chunk = read(handle, remaining)\n","KeyboardInterrupt\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/core/envs.py\", line 508, in _worker\n","    cmd, data = remote.recv()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n","    buf = self._recv_bytes()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n","    buf = self._recv(4)\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n","    chunk = read(handle, remaining)\n","KeyboardInterrupt\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/core/envs.py\", line 508, in _worker\n","    cmd, data = remote.recv()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n","    buf = self._recv_bytes()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n","    buf = self._recv(4)\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n","    chunk = read(handle, remaining)\n","KeyboardInterrupt\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/core/envs.py\", line 508, in _worker\n","    cmd, data = remote.recv()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n","    buf = self._recv_bytes()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n","    buf = self._recv(4)\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n","    chunk = read(handle, remaining)\n","KeyboardInterrupt\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/core/envs.py\", line 508, in _worker\n","    cmd, data = remote.recv()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n","    buf = self._recv_bytes()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n","    buf = self._recv(4)\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n","    chunk = read(handle, remaining)\n","KeyboardInterrupt\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/core/envs.py\", line 508, in _worker\n","    cmd, data = remote.recv()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n","    buf = self._recv_bytes()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n","    buf = self._recv(4)\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n","    chunk = read(handle, remaining)\n","KeyboardInterrupt\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/core/envs.py\", line 515, in _worker\n","    observation, _ = env.reset()\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/train_ppo_in_singleagent_env.py\", line 151, in reset\n","    obs, info = super(SingleAgentRacingEnv, self).reset(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/envs/marl_envs/marl_racing_env.py\", line 384, in reset\n","    ret = super(MultiAgentRacingEnv, self).reset(seed=seed)\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/envs/base_env.py\", line 546, in reset\n","    self.engine.reset()\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/engine/base_engine.py\", line 361, in reset\n","    manager.reset()\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/envs/marl_envs/marl_racing_env.py\", line 331, in reset\n","    _map = self.spawn_object(RacingMap, map_config=config[\"map_config\"], random_seed=None)\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/manager/pg_map_manager.py\", line 27, in spawn_object\n","    map = self.engine.spawn_object(object_class, auto_fill_random_seed=False, force_spawn=True, *args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/engine/base_engine.py\", line 144, in spawn_object\n","    obj = object_class(**kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/component/map/base_map.py\", line 63, in __init__\n","    self._generate()\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/envs/marl_envs/marl_racing_env.py\", line 90, in _generate\n","    last_block = FirstPGBlock(\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/component/pgblock/first_block.py\", line 98, in __init__\n","    self._create_in_world()\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/component/block/base_block.py\", line 229, in _create_in_world\n","    self.create_in_world()\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/component/pgblock/pg_block.py\", line 254, in create_in_world\n","    self._construct_lane(lane, (_from, _to, _id))\n","  File \"/usr/local/lib/python3.10/dist-packages/metadrive/component/block/base_block.py\", line 448, in _construct_lane\n","    segment_node.addShape(shape)\n","KeyboardInterrupt\n","    obs, reward, done, info = envs.step(cpu_actions)\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/core/envs.py\", line 297, in step\n","    return self.step_wait()\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/core/envs.py\", line 616, in step_wait\n","    results = [remote.recv() for remote in self.remotes]\n","  File \"/content/gdrive/MyDrive/cs260r/assignment4/core/envs.py\", line 616, in <listcomp>\n","    results = [remote.recv() for remote in self.remotes]\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n","    buf = self._recv_bytes()\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n","    buf = self._recv(4)\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n","    chunk = read(handle, remaining)\n","KeyboardInterrupt\n"]}]},{"cell_type":"code","source":["# TRAIN AGENT FROM SCRATCH, TEST VEL\n","# %%shell\n","# python train_ppo_in_singleagent_env.py \\\n","# --log-dir train_ppo_in_singleagent_env/ppo_vel_10 \\\n","# --num-processes 10 \\\n","# --num-steps 4_000 \\\n","# --max-steps 1_500_000\n","# --vel-reward 2\n"],"metadata":{"id":"lSS-Yndby4uR"},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}